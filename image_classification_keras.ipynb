{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AIm of this analysis is to develop a strong image classifer using less training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,regularizers\n",
    "from keras.preprocessing .image import ImageDataGenerator\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt   \n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras import regularizers\n",
    "import cv2\n",
    "#FinalModel.add(modelNew)\n",
    "from keras import regularizers\n",
    "from keras import layers\n",
    "from keras.callbacks import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training and validation directories will have following structure. Inspiration for this notebook comes from this [Keras blog post](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) and the VGG ConvNet paper.\n",
    "There are training and test directories and have their corresponding directories as per image class categories. we will use two sets of pictures, which we got from [Kaggle](https://www.kaggle.com/c/dogs-vs-cats/data): 1000 cats and 1000 dogs (although the original dataset had 12,500 cats and 12,500 dogs, we just took the first 1000 images for each class). We will also use 400 additional samples from each class as validation data, to evaluate our model performance.\n",
    "This is quite common in real-time world where we don't get too many data and hence we need to ensure our model is well trained with such minimal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first define the training and testing directory path and store all the images names of categories into their resoective arrasy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akayal\\Devapps\n",
      "Path Is: ../input/train/\n",
      "No of Training Images of Dog: 11252\n",
      "No of Training Images of Cats: 11733\n",
      "No of Test Images: 12500\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "print(ROOT_DIR)\n",
    "TRAIN_DIR = '../input/train/'\n",
    "TRAIN_DIR_DOG = '../input/train/dogs'\n",
    "TRAIN_DIR_CATS = '../input/train/cats'\n",
    "VALIDATION_DIR = '../input/validation/'\n",
    "\n",
    "TEST_DIR = '../input/test/'\n",
    "\n",
    "print(\"Path Is:\",TRAIN_DIR)\n",
    "\n",
    "sys.path.append(os.path.join(ROOT_DIR, EXTENSION_PATH))  # To find local version\n",
    "\n",
    "train_dogs =   [TRAIN_DIR_DOG+inputD for inputD in os.listdir(TRAIN_DIR_DOG) if 'dog' in inputD]\n",
    "train_cats =   [TRAIN_DIR_CATS+inputC for inputC in os.listdir(TRAIN_DIR_CATS) if 'cat' in inputC]\n",
    "test_images =  [TEST_DIR+inputT for inputT in os.listdir(TEST_DIR) ]\n",
    "\n",
    "\n",
    "print(\"No of Training Images of Dog:\",len(train_dogs))\n",
    "print(\"No of Training Images of Cats:\",len(train_cats))\n",
    "print(\"No of Test Images:\",len(test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do basic preprocessing now as part of data loading. Data will be first resized and then will be normalised.\n",
    "\n",
    "Keras provides the ImageDataGenerator class that defines the configuration for image data preparation and augmentation. Some of these capabailities are:\n",
    "\n",
    "- Sample-wise standardization.\n",
    "- Feature-wise standardization.\n",
    "- ZCA whitening.\n",
    "- Random rotation, shifts, shear and flips.\n",
    "- Dimension reordering.\n",
    "- Save augmented images to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train_dogs)\n",
    "random.shuffle(train_cats)\n",
    "random.shuffle(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step will be to read the test and training images. Remember that training data are labelled ones and test data as usual does not have labelled ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = 64\n",
    "image_width=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path Is: ../input/train/\n",
      "Found 22985 images belonging to 2 classes.\n",
      "Found 2015 images belonging to 2 classes.\n",
      "<keras_preprocessing.image.DirectoryIterator object at 0x000001B889986748>\n"
     ]
    }
   ],
   "source": [
    "# we create two instances with the same arguments\n",
    "data_gen_args = dict(\n",
    "        #rotation_range=10,\n",
    "       # width_shift_range=0.1,\n",
    "       # height_shift_range=0.1,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "\n",
    "image_datagen_prep = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "print(\"Path Is:\",TRAIN_DIR)\n",
    "train_generator_data = image_datagen_prep.flow_from_directory(directory = TRAIN_DIR,\n",
    "                                                             target_size=(image_width,image_height),\n",
    "                                                             classes=['dogs','cats'],\n",
    "                                                             class_mode='binary',\n",
    "                                                             batch_size=16,\n",
    "                                                             color_mode='rgb')\n",
    "\n",
    "validation_generator_data = image_datagen_prep.flow_from_directory(directory = VALIDATION_DIR,\n",
    "                                                             target_size=(image_width,image_height),\n",
    "                                                             classes=['dogs','cats'],\n",
    "                                                             class_mode='binary',\n",
    "                                                             batch_size=16,\n",
    "                                                             color_mode='rgb')\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "print(train_generator_data)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development\n",
    "We will now start model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dogsvscats_6_conv_layers\"\n",
    "\n",
    "\n",
    "\n",
    "CNNModel = Sequential()\n",
    "\n",
    "#convolution 1st layer\n",
    "droprate=0.5\n",
    "droprateRev=0.7\n",
    "#convolution 1st layer\n",
    "CNNModel.add(Conv2D(32,(3,3), activation='relu', strides=(1, 1), padding='same', input_shape=(64,64,3)))\n",
    "#model.add(Conv2D(32,(3,1), activation='relu', strides=(1, 1), padding='valid',input_shape=input_24)#(input_24)\n",
    "CNNModel.add(BatchNormalization())\n",
    "#CNNModel.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "#CNNModel.add(Dropout(droprate))#3\n",
    "\n",
    "#convolution 2nd layer\n",
    "CNNModel.add(Conv2D(64,(3,3), padding='valid',activation='relu'))\n",
    "CNNModel.add(BatchNormalization())\n",
    "CNNModel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#CNNModel.add(Dropout(droprate))#3\n",
    "\n",
    "#convolution 3rd layer\n",
    "CNNModel.add(Conv2D(128,(3,3), padding='valid',activation='relu'))\n",
    "CNNModel.add(BatchNormalization())\n",
    "CNNModel.add(MaxPooling2D())\n",
    "CNNModel.add(Dropout(droprateRev))#3\n",
    "\n",
    "#convolution 3rd layer\n",
    "CNNModel.add(Conv2D(256,(3,3), padding='valid',activation='relu'))\n",
    "CNNModel.add(BatchNormalization())\n",
    "CNNModel.add(MaxPooling2D())\n",
    "CNNModel.add(Dropout(droprateRev))#3\n",
    "\n",
    "\n",
    "#convolution 3rd layer\n",
    "CNNModel.add(Conv2D(64,(1,1), activation='relu'))\n",
    "CNNModel.add(BatchNormalization())\n",
    "CNNModel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "CNNModel.add(Dropout(droprateRev))#3\n",
    "\n",
    "#Fully connected final layer\n",
    "CNNModel.add(Flatten())\n",
    "   \n",
    "CNNModel.add(Dense(100, activation='relu'))\n",
    "CNNModel.add(Dropout(droprateRev))\n",
    "\n",
    "CNNModel.add(Dense(50, activation='relu',use_bias=False, kernel_initializer='glorot_uniform'))\n",
    "CNNModel.add(Dropout(droprateRev))#3\n",
    "\n",
    "\n",
    "CNNModel.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.002,\n",
    "                              patience=2, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_116 (Conv2D)          (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_115 (Bat (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_117 (Conv2D)          (None, 62, 62, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_116 (Bat (None, 62, 62, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_90 (MaxPooling (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_118 (Conv2D)          (None, 29, 29, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_117 (Bat (None, 29, 29, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_91 (MaxPooling (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_148 (Dropout)        (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_119 (Conv2D)          (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_118 (Bat (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_92 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_149 (Dropout)        (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_120 (Conv2D)          (None, 6, 6, 64)          16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_119 (Bat (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_93 (MaxPooling (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_150 (Dropout)        (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 100)               57700     \n",
      "_________________________________________________________________\n",
      "dropout_151 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 50)                5000      \n",
      "_________________________________________________________________\n",
      "dropout_152 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 469,791\n",
      "Trainable params: 468,703\n",
      "Non-trainable params: 1,088\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNNModel.compile(loss='binary_crossentropy',\n",
    "              optimizer= 'RMSprop',\n",
    "              metrics=['accuracy'])\n",
    "CNNModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "150/150 [==============================] - 398s 3s/step - loss: 2.1284 - acc: 0.4892 - val_loss: 0.9439 - val_acc: 0.4000\n",
      "Epoch 2/15\n",
      "150/150 [==============================] - 374s 2s/step - loss: 1.0744 - acc: 0.5171 - val_loss: 0.8378 - val_acc: 0.3937\n",
      "Epoch 3/15\n",
      "150/150 [==============================] - 381s 3s/step - loss: 0.8608 - acc: 0.5225 - val_loss: 0.9096 - val_acc: 0.3875\n",
      "Epoch 4/15\n",
      "131/150 [=========================>....] - ETA: 47s - loss: 0.7795 - acc: 0.5119"
     ]
    }
   ],
   "source": [
    "history = CNNModel.fit_generator(generator=train_generator_data, \n",
    "                        steps_per_epoch=150,\n",
    "                        epochs=15,\n",
    "                        verbose=1,\n",
    "                        validation_steps=30,  \n",
    "                        validation_data=validation_generator_data,\n",
    "                        callbacks=[reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tflearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-160-6aa740fdf56a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensorboard_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"log\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}.meta\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MODEL-LOADED\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tflearn' is not defined"
     ]
    }
   ],
   "source": [
    "model = tflearn.DNN(convnet, tensorboard_dir=\"log\")\n",
    "if(os.path.exists(\"{}.meta\".format(model_name))):\n",
    "    model.load(model_name)\n",
    "print(\"MODEL-LOADED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For doing predictions this worked for me:\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "test_model = load_model('my_model_name.h5')\n",
    "img = load_img('image_to_predict.jpg',False,target_size=(img_width,img_height))\n",
    "x = img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "preds = test_model.predict_classes(x)\n",
    "prob = test_model.predict_proba(x)\n",
    "print(preds, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the bottleneck features of a pre-trained network. A more refined approach would be to leverage a network pre-trained on a large dataset. We will use the VGG16 architecture, pre-trained on the ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
